{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609e7be4-7a2c-47da-b8c7-488429747d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  sagemaker==2.173.0 --no-deps \\\n",
    "  pinecone-client==2.2.1 --no-deps \\\n",
    "  ipywidgets==8.0.0 --no-deps \\\n",
    "  PyPDF2 --no-deps \\\n",
    "  faiss-cpu --no-deps \\\n",
    "  langchain --no-deps \\\n",
    "  numpy==1.26.4 --no-deps \\\n",
    "  pandas==2.2.3 --no-deps \\\n",
    "  ipython --no-deps \\\n",
    "  ipython_genutils --no-deps \\\n",
    "  jupyterlab_widgets --no-deps \\\n",
    "  widgetsnbextension --no-deps \\\n",
    "  sentence-transformers --no-deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2744262-f1a8-467d-b15a-d8fccc8df6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "from sagemaker.huggingface import(\n",
    "HuggingFaceModel,\n",
    "get_huggingface_llm_image_uri\n",
    ")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "hub_config = {\n",
    "    'HF_MODEL_ID':'google/flan-t5-large',\n",
    "    'HF_TASK':'text-generation'\n",
    "}\n",
    "\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    \"huggingface\",\n",
    "    version=\"0.8.2\"\n",
    ")\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub_config,\n",
    "    role=role,\n",
    "    image_uri=llm_image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fd6365-a8df-4978-aeb8-2cc57dcde69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "llm = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    endpoint_name=\"flan-t5-demo2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85ec5c6c-085d-4e10-8e7a-e3df986682c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# 👇 Rebuild your predictor properly\n",
    "llm = Predictor(\n",
    "    endpoint_name=\"flan-t5-demo2\",\n",
    "    serializer=JSONSerializer(),           # sends JSON with correct Content-Type\n",
    "    deserializer=JSONDeserializer()        # parses JSON response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98d0b41-2fc0-40d5-adcd-eeb244275d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'bubbles'}]\n"
     ]
    }
   ],
   "source": [
    "question = \"What is bubble sort\"\n",
    "response = llm.predict({\"inputs\": question})\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a58a989-030a-46ca-8ab4-ed4e1cae57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Managed Spot training can be used with all instances supported in Amazon SageMaker. Managed Spot Training is supported in all AWS Regions where Amazon SageMaker is currently available\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337f6df9-3f3c-43b2-a29a-429a72403cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: What is bubble sort\n",
      "[Output]: I don't know\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"Answer the following QUESTION based on the CONTEXT given. If you do not know the answer and the CONTEXT doesn't contain the answer truthfully say \"I don't know\".\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\",context).replace(\"{question}\",question) \n",
    "out = llm.predict({\"inputs\":text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {question}\\n[Output]: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2085fdb2-7d3c-4a27-a18d-37bf62169804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input]: What color is my desk?\n",
      "[Output]: I don't know\n"
     ]
    }
   ],
   "source": [
    "unanswerable_question = \"What color is my desk?\"\n",
    "\n",
    "text_input = prompt_template.replace(\"{context}\",context).replace(\"{question}\",unanswerable_question) \n",
    "out = llm.predict({\"inputs\":text_input})\n",
    "generated_text = out[0][\"generated_text\"]\n",
    "print(f\"[Input]: {unanswerable_question}\\n[Output]: {generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c319ae-c5e5-48ba-8173-1cc57a99aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_config = {\n",
    "    'HF_MODEL_ID': 'sentence-transformers/all-MiniLM-L6-v2', \n",
    "    'HF_TASK':'feature-extraction'\n",
    "}\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub_config,\n",
    "    role = role,\n",
    "    transformers_version=\"4.6\",\n",
    "    pytorch_version=\"1.7\",\n",
    "    py_version=\"py36\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd8f3b02-3f1e-42cc-8300-b4b211c254ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "encoder = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.t2.large\",\n",
    "    endpoint_name=\"minilm-demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7098582-23ca-47e0-bf9e-e23787159abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d17bef7a-2c9f-4c37-9323-796866a87bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Result for chunk 1: [0.26777422428131104, -0.12856808304786682, -0.25632742047309875, 0.005600318778306246, -0.46683451533317566]...\n",
      "✅ Result for chunk 2: [0.07167105376720428, -0.015892410650849342, -0.16532552242279053, -0.02619650401175022, -0.4145868122577667]...\n",
      "✅ Result for chunk 3: [-0.058464471250772476, 0.0032196284737437963, -0.3308429718017578, 0.2719138562679291, -0.19384631514549255]...\n",
      "✅ Result for chunk 4: [0.05875478684902191, 0.016153700649738312, -0.08368296176195145, 0.038199614733457565, -0.6158127188682556]...\n",
      "✅ Result for chunk 5: [0.34844034910202026, -0.1921585202217102, -0.15035594999790192, 0.05665930360555649, -0.341560423374176]...\n",
      "✅ Result for chunk 6: [0.3743951916694641, -0.2476978749036789, -0.14139395952224731, 0.31055647134780884, -0.1806740164756775]...\n",
      "✅ Result for chunk 7: [0.2918042242527008, -0.09482786804437637, -0.03357619047164917, -0.21713602542877197, 0.05217383801937103]...\n",
      "✅ Result for chunk 8: [0.18471424281597137, -0.2693515121936798, -0.09486933052539825, -0.2059246450662613, -0.2446928173303604]...\n",
      "✅ Result for chunk 9: [-0.019772032275795937, 0.007300489582121372, -0.15474124252796173, -0.16277965903282166, -0.3643432855606079]...\n",
      "✅ Result for chunk 10: [-0.0075884792022407055, -0.1983145773410797, -0.4024452567100525, -0.08923213183879852, -0.17925919592380524]...\n",
      "✅ Result for chunk 11: [-0.01821463368833065, 0.2500848174095154, 0.11193172633647919, -0.055261727422475815, -0.21510381996631622]...\n",
      "✅ Result for chunk 12: [0.10302457213401794, 0.3425408899784088, -0.08671099692583084, 0.015128379687666893, -0.18638622760772705]...\n",
      "✅ Result for chunk 13: [0.358011931180954, -0.2550707757472992, -0.14033086597919464, 0.08938293904066086, -0.16593089699745178]...\n",
      "✅ Result for chunk 14: [0.1467326432466507, -0.2516622245311737, -0.18514089286327362, 0.20623698830604553, -0.3151344656944275]...\n",
      "✅ Result for chunk 15: [0.07346470654010773, 0.03606905788183212, -0.1321529895067215, 0.09236747026443481, 0.025246821343898773]...\n",
      "✅ Result for chunk 16: [0.24936111271381378, -0.08611492067575455, -0.15547408163547516, 0.2109161913394928, -0.03819004073739052]...\n",
      "✅ Result for chunk 17: [0.31476065516471863, -0.13740572333335876, -0.2327239215373993, 0.12054171413183212, -0.18131527304649353]...\n",
      "✅ Result for chunk 18: [0.566281795501709, -0.1261683702468872, -0.1729128211736679, 0.15014322102069855, -0.10324176400899887]...\n",
      "✅ Result for chunk 19: [0.07972580939531326, -0.39041322469711304, 0.004473807290196419, 0.2891383171081543, -0.17106589674949646]...\n",
      "✅ Result for chunk 20: [0.08547820150852203, 0.165590301156044, 0.1703491061925888, -0.15304140746593475, -0.15993033349514008]...\n",
      "✅ Result for chunk 21: [0.055341657251119614, 0.017757469788193703, -0.16860315203666687, 0.17840269207954407, -0.10856876522302628]...\n",
      "✅ Result for chunk 22: [-0.08759666234254837, 0.06874528527259827, 0.017095666378736496, -0.29943031072616577, -0.2675270140171051]...\n",
      "✅ Result for chunk 23: [0.128739595413208, -0.07450532168149948, -0.14651338756084442, -0.22727413475513458, -0.22758273780345917]...\n",
      "✅ Result for chunk 24: [0.020755764096975327, 0.09383188188076019, -0.056142259389162064, -0.13654157519340515, -0.1703050285577774]...\n",
      "✅ Result for chunk 25: [-0.005226847715675831, -0.03826509043574333, 0.06764190644025803, -0.12642307579517365, -0.07717949151992798]...\n",
      "✅ Result for chunk 26: [-0.13637983798980713, 0.04439505934715271, -0.12921205163002014, 0.032906752079725266, 0.035799577832221985]...\n",
      "✅ Result for chunk 27: [-0.19335538148880005, -0.21485748887062073, -0.19521859288215637, 0.3931220471858978, -0.04819507524371147]...\n",
      "✅ Result for chunk 28: [-0.0654863491654396, -0.005970702040940523, -0.18729470670223236, 0.4088752865791321, 0.22938679158687592]...\n",
      "✅ FAISS index built with 28 chunks.\n",
      "\n",
      "📋 Answering questions:\n",
      "⏱️ Time taken for processing: 0.86 seconds\n",
      "\n",
      "Q: What is rollback in ACID property\n",
      "A: I don't know\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sagemaker import Session\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import time  # Import the time module\n",
    "\n",
    "# --- Set up SageMaker Predictor ---\n",
    "endpoint_name = \"minilm-demo\"  # 🔁 Replace with your actual endpoint name\n",
    "session = Session()\n",
    "\n",
    "encoder = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# --- Load PDF from same folder ---\n",
    "pdf_filename = \"24042020_Transaction Management in DBMS.pdf\"  # Update to the path of the sample PDF\n",
    "pdf_path = os.path.join(os.getcwd(), pdf_filename)\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "pdf_text = \"\\n\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
    "\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_text(pdf_text)\n",
    "\n",
    "# --- Generate Embeddings ---\n",
    "valid_embeddings = []\n",
    "valid_chunks = []\n",
    "invalid_chunks = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    if chunk.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        result = encoder.predict({\"inputs\": [chunk]})\n",
    "\n",
    "        # Unwrap double list layer\n",
    "        if isinstance(result, list) and isinstance(result[0], list):\n",
    "            embedding = result[0][0]\n",
    "        else:\n",
    "            print(f\"❌ Unexpected embedding format for chunk {idx + 1}: {result}\")\n",
    "            invalid_chunks.append(chunk)\n",
    "            continue\n",
    "\n",
    "        # Debug partial print\n",
    "        print(f\"✅ Result for chunk {idx + 1}: {embedding[:5]}...\")\n",
    "\n",
    "        if all(isinstance(x, (int, float)) for x in embedding):\n",
    "            valid_embeddings.append(embedding)\n",
    "            valid_chunks.append(chunk)\n",
    "        else:\n",
    "            print(f\"❌ Invalid embedding values for chunk {idx + 1}\")\n",
    "            invalid_chunks.append(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating embedding for chunk {idx + 1}: {e}\")\n",
    "        invalid_chunks.append(chunk)\n",
    "\n",
    "if not valid_embeddings:\n",
    "    print(f\"❌ No valid embeddings generated. Invalid chunks: {len(invalid_chunks)}\")\n",
    "    raise ValueError(\"No valid embeddings generated.\")\n",
    "\n",
    "# --- Build FAISS index ---\n",
    "dimension = len(valid_embeddings[0])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(valid_embeddings).astype(\"float32\"))\n",
    "print(f\"✅ FAISS index built with {len(valid_embeddings)} chunks.\")\n",
    "\n",
    "# --- Context Retrieval ---\n",
    "def retrieve_context(question):\n",
    "    result = encoder.predict({\"inputs\": [question]})\n",
    "    question_embedding = result[0][0]  # Same unwrapping here\n",
    "    D, I = index.search(np.array([question_embedding]).astype(\"float32\"), k=1)\n",
    "    return valid_chunks[I[0][0]]\n",
    "\n",
    "# --- Answer Generation ---\n",
    "def generate_answer(question):\n",
    "    # Start the timer when the function is called\n",
    "    start_time = time.time()\n",
    "\n",
    "    context = retrieve_context(question)\n",
    "    text_input = prompt_template.replace(\"{context}\", context).replace(\"{question}\", question)\n",
    "    out = llm.predict({\"inputs\": text_input})\n",
    "    answer = out[0][\"generated_text\"]\n",
    "\n",
    "    # Stop the timer and calculate elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time  # in seconds\n",
    "\n",
    "    # Print the time taken for processing\n",
    "    print(f\"⏱️ Time taken for processing: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    return answer\n",
    "\n",
    "# --- Sample Questions ---\n",
    "questions = [\n",
    "    \"What is rollback in ACID property\"\n",
    "]\n",
    "\n",
    "print(\"\\n📋 Answering questions:\")\n",
    "for q in questions:\n",
    "    ans = generate_answer(q)\n",
    "    print(f\"\\nQ: {q}\\nA: {ans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b81714-302d-49ab-8d4b-e777632ed912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
