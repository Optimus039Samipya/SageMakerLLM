🧠 **PDF Question Answering with SageMaker + FAISS**
This project builds a lightweight semantic Question Answering (QA) system that allows users to ask natural language questions based on the contents of a PDF. It combines text embedding, vector search, and text generation using Amazon SageMaker endpoints.

🚀 **Features**
📄 Extracts text from PDFs using PyPDF2

✂️ Splits text into manageable chunks with LangChain

🔍 Generates embeddings using a MiniLM encoder via SageMaker

🧠 Stores embeddings in a FAISS vector index for fast similarity search

🤖 Answers questions using Flan-T5 (or any text generation model) hosted on SageMaker

⏱️ Measures inference time per question

⚙️ **How It Works**
_PDF Ingestion_
Loads a local PDF and extracts all textual content.

_Text Splitting_
Chunks the text into 500-character blocks with 50-character overlap to maintain context.

_Embedding Generation_
Each chunk is embedded using a MiniLM model deployed on SageMaker.

_Invalid or empty chunks are filtered out._

_Vector Indexing_
Embeddings are stored in a FAISS index (IndexFlatL2) to enable fast nearest-neighbor search.

_Context Retrieval + Answer Generation_
A user question is embedded and compared to the indexed chunks.

The most relevant chunk is passed as context to a generative model (e.g., Flan-T5) to produce an answer.

🛠️ **Dependencies**
PyPDF2

langchain

faiss

numpy

boto3, sagemaker SDK

📝**Sample Question**
question = "What is rollback in ACID property?"
answer = generate_answer(question)
Q: What is rollback in ACID property?
A: (Answer generated by the model based on the retrieved context)

🧪**SageMaker Endpoints**
minilm-demo: Encoder for semantic embeddings

flan-t5-qa: Generator for answering based on retrieved context

✅ Make sure these endpoints are active in your AWS account before running.
