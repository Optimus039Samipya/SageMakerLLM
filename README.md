ğŸ§  **PDF Question Answering with SageMaker + FAISS**
This project builds a lightweight semantic Question Answering (QA) system that allows users to ask natural language questions based on the contents of a PDF. It combines text embedding, vector search, and text generation using Amazon SageMaker endpoints.

ğŸš€ **Features**
ğŸ“„ Extracts text from PDFs using PyPDF2

âœ‚ï¸ Splits text into manageable chunks with LangChain

ğŸ” Generates embeddings using a MiniLM encoder via SageMaker

ğŸ§  Stores embeddings in a FAISS vector index for fast similarity search

ğŸ¤– Answers questions using Flan-T5 (or any text generation model) hosted on SageMaker

â±ï¸ Measures inference time per question

âš™ï¸ **How It Works**
_PDF Ingestion_
Loads a local PDF and extracts all textual content.

_Text Splitting_
Chunks the text into 500-character blocks with 50-character overlap to maintain context.

_Embedding Generation_
Each chunk is embedded using a MiniLM model deployed on SageMaker.

_Invalid or empty chunks are filtered out._

_Vector Indexing_
Embeddings are stored in a FAISS index (IndexFlatL2) to enable fast nearest-neighbor search.

_Context Retrieval + Answer Generation_
A user question is embedded and compared to the indexed chunks.

The most relevant chunk is passed as context to a generative model (e.g., Flan-T5) to produce an answer.

ğŸ› ï¸ **Dependencies**
PyPDF2

langchain

faiss

numpy

boto3, sagemaker SDK

ğŸ“**Sample Question**
question = "What is rollback in ACID property?"
answer = generate_answer(question)
Q: What is rollback in ACID property?
A: (Answer generated by the model based on the retrieved context)

ğŸ§ª**SageMaker Endpoints**
minilm-demo: Encoder for semantic embeddings

flan-t5-qa: Generator for answering based on retrieved context

âœ… Make sure these endpoints are active in your AWS account before running.
